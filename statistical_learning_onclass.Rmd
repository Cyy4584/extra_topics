---
title: "statistical learning"
author: "Yingyu Cui"
date: "2024-11-21"
output: github_document
---

```{r setup, include=FALSE}
library(glmnet)
library(tidyverse)

set.seed(1)
```

# Lasso and regression: the number of covariances and automaticallt predictable and the drawbacks of lasso and nearly all other stats learning methods

# The statistical learning is a combination of cross validation and lasso 

# clusering: method --- k-means?

# Now try Lasso
```{r lasso}
bwt_df = 
  read_csv("data/birthweight.csv") |> 
  janitor::clean_names() |> 
   mutate(
    babysex = 
        case_match(babysex,
            1 ~ "male",
            2 ~ "female"
        ),
    babysex = fct_infreq(babysex),
    frace = 
        case_match(frace,
            1 ~ "white",
            2 ~ "black", 
            3 ~ "asian", 
            4 ~ "puerto rican", 
            8 ~ "other"),
    frace = fct_infreq(frace),
    mrace = 
        case_match(mrace,
            1 ~ "white",
            2 ~ "black", 
            3 ~ "asian", 
            4 ~ "puerto rican",
            8 ~ "other"),
    mrace = fct_infreq(mrace),
    malform = as.logical(malform)) |> 
  sample_n(200)

# when you have different sample you get diiferent results, so you need to set seed
```


# construct inputs for glmnet
```{r glmnet}
x = model.matrix(bwt ~ ., bwt_df)[,-1]
y = bwt_df |> pull(bwt)

```

# fit lasso for several values of lambda
```{r fit_lasso}
lambda = 10^(seq(-2, 2.75, 0.1)) # which is tried by Jeff, how I set the lambda?

lasso_fit =
  glmnet(x, y, lambda = lambda)

lasso_cv =
  cv.glmnet(x = x , y = y, lambda = lambda)
# this step is to find the optimal lambda, the plot below just a visualization 

#look at this cv
lasso_cv |> 
  broom::tidy() |> 
  ggplot(aes(x = log(lambda, 10), y = estimate)) + 
  geom_point() 

lambda_opt = lasso_cv[["lambda.min"]]
# select the best lambda and pull out

# usual lasso fit plot view 
lasso_fit |> 
  broom::tidy() |> 
  select(term, lambda, estimate) |> 
  complete(term, lambda, fill = list(estimate = 0) ) |> 
  filter(term != "(Intercept)") |> 
  ggplot(aes(x = lambda, y = estimate, group = term, color = term)) + 
  geom_path() + 
  geom_vline(xintercept = lambda_opt, color = "blue", size = 1.2) +
  theme(legend.position = "none")
```

# fit lasso with optimal lambda
```{r fit_lasso_opt}
lasso_fit = 
  glmnet(x, y, lambda = lambda_opt)

lasso_fit |> broom::tidy()
```


# clustering
```{r clustering}
# k-means clustering
poke_df = 
  read_csv("data/pokemon.csv") |> 
  janitor::clean_names() |> 
  select(hp, speed)

poke_df |> 
  ggplot(aes(x = hp, y = speed)) + 
  geom_point()

kmeans_fit =
  kmeans(x = poke_df, centers = 3)
# this is decided by the columns you choose

poke_df =
  broom::augment(kmeans_fit, poke_df)

poke_df |> 
  ggplot(aes(x = hp, y = speed, color = .cluster)) +
  geom_point()


# we could use map to do the same thing for different k
clusts =
  tibble(k = 2:4) |>
  mutate(
    km_fit =    map(k, \(n_clust) kmeans(poke_df, centers = n_clust)),
    augmented = map(km_fit, \(fit) broom::augment(x = fit, poke_df))
  )

clusts |> 
  select(-km_fit) |> 
  unnest(augmented) |> 
  ggplot(aes(hp, speed, color = .cluster)) +
  geom_point(aes(color = .cluster)) +
  facet_grid(~k)

```


